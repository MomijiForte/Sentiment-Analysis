{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44281064",
   "metadata": {},
   "source": [
    "### Coursework 2\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. \n",
    "Both the classification tasks relate to text classification tasks. \n",
    "\n",
    "One task is to be solved using Support Vector Machines. The other has to be solved using Boosting.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single zip file. You could have additional functions implemented that you require for carrying out each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ffe46",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train an SVM based classifier to obtain train and check on the sample test dataset provided. The method will be evaluated also against an external test set. Please do not hardcode any dimensions or number of samples while writing the code. It should be possible to automate the testing and hardcoding values does not allow for automated testing. \n",
    "\n",
    "You are allowed to use scikit-learn to implement the SVM. However, you are expected to write your own kernels.\n",
    "\n",
    "You are allowed to use the existing library functions such as scikit-learn or numpy for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. Refer to the documentation provided [here](https://scikit-learn.org/stable/modules/svm.html) at 1.4.6.2 and an example [here](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html) for writing your own kernels.\n",
    "\n",
    "Details regarding the marking have been provided in the coursework specification file. Ensure that the code can be run with different test files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385ce53",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f27e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ac481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords') #download nltk natural packages\n",
    "    from nltk.corpus import stopwords, wordnet as wn #used to exclude unnecessary punctuations or words\n",
    "    from nltk import word_tokenize, pos_tag #used to divide sentence into each individual strings\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    wnl = WordNetLemmatizer() #create object to hold lemmatizer word\n",
    "    from collections import defaultdict\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['j'] = wn.ADJ\n",
    "    tag_map['v'] = wn.VERB\n",
    "    tag_map['a'] = wn.ADV\n",
    "\n",
    "    def Clean_Data(review):\n",
    "        review = review.lower() #converts in to lower case\n",
    "        review= re.sub('\\[.*?\\]', '', review) #removes characters inside []form each reviews\n",
    "        review = re.sub(\"\\\\W\",\" \",review) #removes /,\\ slashes \n",
    "        review = re.sub('https?://\\S+|www\\.\\S+', '', review) #remove the links and macthes the white spaces\n",
    "        review = re.sub('<.*?>+', '', review) #removes <br> tags specially <>\n",
    "        review = re.sub('br','',review)\n",
    "        #review = re.sub('[%s]' % re.escape(string.punctuation), '', review) #remove punctuations from the review\n",
    "        review = re.sub('[^\\w\\s]','',review)\n",
    "        review= re.sub('\\n', '', review) #matches the white spaces and reformat the text\n",
    "        review = re.sub('\\w*\\d\\w*', '', review) #remove numbers and digits from the reviews   \n",
    "        return review\n",
    "\n",
    "    def nltk_pos_tagger(nltk_tag):\n",
    "        # checking the context of whole sentence and determine each term is what part of speech\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:          \n",
    "            return None\n",
    "\n",
    "    def lemmatize_sentence(sentence):\n",
    "        nltk_tagged = nltk.pos_tag(sentence) #apply nltk default packages for tokenizing each word \n",
    "        wordnet_tagged = map(lambda x: (x[0], nltk_pos_tagger(x[1])), nltk_tagged) #map the pos tag with corresponding term\n",
    "        lemmatized_sentence = []\n",
    "        \n",
    "        #loop through tagged term and append them correspondingly vased on the tag\n",
    "        for word, tag in wordnet_tagged:\n",
    "            if tag is None:\n",
    "                lemmatized_sentence.append(word) # if tag is none, append the tag\n",
    "            else:        \n",
    "                lemmatized_sentence.append(wnl.lemmatize(word, tag)) #if not none, append the tag and the term together\n",
    "        return \" \".join(lemmatized_sentence)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df = 5, #remove words that appear too rarely\n",
    "                               max_df = 0.8, #remove words that appear too many\n",
    "                               sublinear_tf = True,\n",
    "                               use_idf = True,  \n",
    "                               ngram_range = (1,2), #allow the use of unigrams and bigrams (two word combinations)\n",
    "                               max_features = 10000) #setting maximum features to prevent overfitting\n",
    "\n",
    "    '''Read the CSV file and extract Bag of Words Features'''\n",
    "    Data_train = pd.read_csv(train_file) #'movie_review_train.csv'\n",
    "    Data_test = pd.read_csv(test_file) #'movie_review_test.csv'\n",
    "    pd.set_option('display.max_colwidth',5000)\n",
    "\n",
    "    '''Set our list of stopwords as those from the Natural Language Toolkit for the english language'''\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    '''Preprocessing Data for Train file'''\n",
    "    Data_train['review'] = Data_train['review'].apply(Clean_Data)\n",
    "\n",
    "    '''Use the Natural Language Toolkit to tokenise the clean reviews'''\n",
    "    Data_train['review'] = Data_train['review'].apply(nltk.word_tokenize) \n",
    "\n",
    "    '''Remove all stopwords from our tokenised list'''\n",
    "    Data_train['review'] = Data_train['review'].apply(lambda x: [word for word in x if word not in stopwords])\n",
    "\n",
    "    '''Apply lemmatization and tokenise the results'''\n",
    "    Data_train['review_token'] = Data_train['review'].apply(lemmatize_sentence)\n",
    "    \n",
    "    '''Create a new column illustraing the sentiment type as int'''\n",
    "    Data_train['Value'] = Data_train['sentiment'].apply(lambda x: 1 if x =='positive' else -1)\n",
    "    \n",
    "    '''Preprocessing Data for Test file'''\n",
    "    Data_test['review'] = Data_test['review'].apply(Clean_Data)\n",
    "    \n",
    "    '''Use the Natural Language Toolkit to tokenise the clean reviews'''\n",
    "    Data_test['review'] = Data_test['review'].apply(nltk.word_tokenize) \n",
    "    \n",
    "    '''Remove all stopwords from our tokenised list'''\n",
    "    Data_test['review'] = Data_test['review'].apply(lambda x: [word for word in x if word not in stopwords])\n",
    "    \n",
    "    '''Apply lemmatization and tokenise the results'''\n",
    "    Data_test['review_token'] = Data_test['review'].apply(lemmatize_sentence)\n",
    "    \n",
    "    '''Create a new column illustraing the sentiment type as int'''\n",
    "    Data_test['Value'] = Data_test['sentiment'].apply(lambda x: 1 if x =='positive' else -1)\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(Data_train['review_token'])\n",
    "    y_train = np.array(Data_train[\"Value\"])\n",
    "    \n",
    "    X_test = vectorizer.transform(Data_test['review_token'])\n",
    "    y_test = np.array(Data_test[\"Value\"])\n",
    "    \n",
    "    return (X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94c07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self):\n",
    "        import numpy as np\n",
    "        from sklearn import svm\n",
    "        #implement initialisation\n",
    "        self.some_paramter=1\n",
    "        #Calling SVM kernel with some hyperparameter\n",
    "        self.clf = svm.SVC(kernel=self.laplacian_kernel, degree = 2, C = 1)\n",
    "        \n",
    "    # define your own kernel here\n",
    "    # Refer to the documentation here: https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html \n",
    "    '''Refer to documentation here for how I create the code: \n",
    "    http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/#source_code'''\n",
    "    \n",
    "    def laplacian_kernel(self, X, y):\n",
    "        import numpy as np\n",
    "        from sklearn.metrics.pairwise import euclidean_distances\n",
    "        import scipy\n",
    "        \n",
    "        # Calculating Standard Deviation Sigma\n",
    "        if scipy.sparse.issparse(X): # Checking if X_train is a matrix\n",
    "            sigma = np.sqrt(X.shape[1] * np.var(X.toarray())) #if yes, then caluclate sigma but variance have to be an array\n",
    "        else:\n",
    "            sigma = np.sqrt(X.shape[1] * np.var(X))\n",
    "\n",
    "        euclid_dist = euclidean_distances(X, y) #using euclidean_distances function to find the distance between two points\n",
    "        result = (- euclid_dist/ sigma) # follow the equations from the reference website above\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def cauchy_kernel(self, X, y):\n",
    "        import numpy as np\n",
    "        from sklearn.metrics.pairwise import euclidean_distances\n",
    "        import scipy\n",
    "        \n",
    "        # Calculating Standard Deciation Sigma\n",
    "        if scipy.sparse.issparse(X): #Checking if X_train is a matrix\n",
    "            sigma = X.shape[1] * np.var(X.toarray()) # if yes, then calculate sigma but variacne have to be an array\n",
    "        else:\n",
    "            sigma = X.shape[1] * np.var(X)\n",
    "            \n",
    "        euclid_dist = euclidean_distances(X, y) # using euclidean_distances to find the distance between two points\n",
    "        result = 1 / (1+ (euclid_dist**2 / sigma**2)) #follow the equations from the reference website above\n",
    "        return result\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # fitting the model\n",
    "        f = self.clf.fit(X,Y)\n",
    "        return f\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        prediction = self.clf.predict(X)\n",
    "        return prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6f272",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89603f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score  \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit(X_train, Y_train)\n",
    "    Y_Pred = sc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    print(\"Accuracy:\",acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ffd4adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.874\n"
     ]
    }
   ],
   "source": [
    "acc = test_func_svm(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61056292",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "In this task you need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "Details regarding the marking for the coursework are provided in the coursework specification file. Please ensure that your code will work with a different test file than the one provided with the coursework.\n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from scikit-learn or your own decision trees. Use the same sentiment analysis dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3805e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingClassifier:\n",
    "    # You need to implement this classifier. \n",
    "    def __init__(self):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        import numpy as np\n",
    "        #implement initialisation\n",
    "        self.some_paramter=1\n",
    "        # Calling DecistionTree classifier with its hyperparameter\n",
    "        self.model = DecisionTreeClassifier(criterion = 'entropy', max_depth=7)\n",
    "        self.classifiers = []\n",
    "        self.alphas = []\n",
    "        \n",
    "    def fit(self, X,y):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.metrics import accuracy_score \n",
    "        from sklearn import base\n",
    "        import numpy as np\n",
    "        # calling X shape parameter\n",
    "        N, M = X.shape\n",
    "        #calculaing the weights based on the X_train parameter\n",
    "        weights = np.full(N, (1 / N), dtype=np.float32)\n",
    "        #Append correct classifier to classifiers list\n",
    "        classifiers = []\n",
    "        #Append incorrect classifier to Alphas list\n",
    "        alphas = []\n",
    "        \n",
    "        #looping the maximum number of trees\n",
    "        for i in range(1000):\n",
    "            \n",
    "            #Cloning the fitted decision tree model with the weight scale we caluclate above\n",
    "            f = base.clone(self.model).fit(X,y, sample_weight=weights)\n",
    "            \n",
    "            # Add decision tree to list of classifiers\n",
    "            classifiers.append(f)\n",
    "            \n",
    "            # Use our weak learner to generate a prediction\n",
    "            pred = f.predict(X)\n",
    "            \n",
    "            # Calculate the accuracy of our prediction\n",
    "            acc = accuracy_score(y, pred)\n",
    "            \n",
    "            # Create a list of entries where the prediction was incorrect\n",
    "            incorrect = np.where(pred != y, 1, 0)\n",
    "            \n",
    "            # Calculate the error\n",
    "            error = np.sum(weights * incorrect)\n",
    "            \n",
    "            # Calculate the alpha\n",
    "            a = np.log((1 - error) / error) / 2\n",
    "            \n",
    "            # Add alpha to our existing list of alphas\n",
    "            alphas.append(a)\n",
    "            \n",
    "            # Calculate our new sample weights\n",
    "            weights = weights * np.exp(-a * y * pred)\n",
    "            \n",
    "            # Normalise so that weights sum to 1\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "        #implement training of the boosting classifier\n",
    "        self.classifiers = classifiers\n",
    "        self.alphas = alphas\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        import numpy as np\n",
    "        # implement prediction of the boosting classifier\n",
    "        prediction = np.zeros(X.shape[0])\n",
    "        classifiers = self.classifiers\n",
    "        alphas = self.alphas\n",
    "        \n",
    "        # Loop through our list of classifiers and corresponding alphas\n",
    "        for i in range(len(alphas)):\n",
    "            \n",
    "            # Generate our weighted prediction\n",
    "            prediction += alphas[i] * classifiers[i].predict(X)\n",
    "        \n",
    "        # Determine whether the weighted prediction is overall positive or negative\n",
    "        prediction = np.sign(prediction)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e0987",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4632591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    Y_Pred = bc.predict(X_test)    \n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    print(\"Accuracy:\",acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6c27de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.838\n"
     ]
    }
   ],
   "source": [
    "acc = test_func_boosting(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23585122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords') #download nltk natural packages\n",
    "# from nltk.corpus import stopwords #used to exclude unnecessary punctuations or words\n",
    "# stop = stopwords.words('english') #access default stopwords in nltk packages\n",
    "# print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df339276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
